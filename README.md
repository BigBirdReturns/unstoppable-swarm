# ğŸ›°ï¸ Unstoppable v2 â€“ Open Source AI Swarm Framework

A local-first, agent-coordinated, modular AI orchestration system designed for real-world transparency, resilience, and **zero dependency on cloud services**.

---

## âœ… System Status

This repo contains the **working implementation** of Unstoppable v2:

- ğŸ” **Planner â†’ Coder â†’ Verifier** agent loop
- ğŸ§  **Spectra Rehydration Paradigm** (compressed vector memory â†’ rehydrated answers)
- ğŸ§® **LLM inference powered by llama.cpp** (offline `.gguf` model)
- ğŸ§¾ Fully inspectable prompt construction + output verification
- ğŸ’» Zero external API calls. **All execution is local**.
- ğŸ”§ Runs on CPU or GPU using quantized models like `mistral-7b-instruct-v0.2.Q4_K_M.gguf`

---

## ğŸ“‚ Directory Structure

```
unstoppable-swarm/
â”œâ”€â”€ cli.py                     # Main entry point for swarm execution
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ .gitignore
â”œâ”€â”€ models/                    # Place your .gguf model here (e.g. mistral-7b-instruct-v0.2.Q4_K_M.gguf)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ docs/                  # Optional custom doctrine fragments
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/                # All agent logic
â”‚   â”‚   â”œâ”€â”€ planner_agent.py
â”‚   â”‚   â”œâ”€â”€ coder_agent.py     â† Uses rehydration + LLM inference
â”‚   â”‚   â”œâ”€â”€ verifier_agent.py
â”‚   â”‚   â””â”€â”€ llm.py             â† llama.cpp wrapper
â”‚   â”œâ”€â”€ orchestrator/
â”‚   â”‚   â”œâ”€â”€ orchestrator.py
â”‚   â”‚   â””â”€â”€ agent_interface.py
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”œâ”€â”€ ingest.py          â† Loads doctrine into Chroma vector DB
â”‚   â”‚   â”œâ”€â”€ vector_store.py    â† Chroma wrapper
â”‚   â”‚   â””â”€â”€ rehydrate.py       â† Query+prompt builder
```

---

## ğŸš€ How to Run It

### 1. Install requirements (in a fresh `.venv`)
```bash
pip install -r requirements.txt
```

Or manually:
```bash
pip install llama-cpp-python chromadb sentence-transformers numpy
```

### 2. Download a `.gguf` model
Recommended:
- [`mistral-7b-instruct-v0.2.Q4_K_M.gguf`](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF)

Place it in:
```
models/mistral-7b-instruct-v0.2.Q4_K_M.gguf
```

### 3. Ingest doctrine into memory
```bash
python -m src.memory.ingest
```

### 4. Run the CLI swarm
```bash
python cli.py
```

---

## ğŸ§  Example Prompt

```text
> Explain Spectraâ€™s collapse-and-rehydration memory design in plain English.
```

âœ”ï¸ Context will be retrieved from vector memory  
âœ”ï¸ Coder agent rehydrates it into a full answer  
âœ”ï¸ Verifier signs off on the result  

---

## ğŸ›¤ï¸ Roadmap

The following components are modular and will be layered in next:

- ğŸ§¾ ITL Ledger (trustless append-only log)
- ğŸ§  Field Zero drift/fork/tension tracker
- ğŸ–¥ï¸ Streamlit UI (multi-agent dashboard)
- ğŸ§© Persona overlays (LoRA adapters per agent)

---

## ğŸ§­ Philosophy

No prompt-stuffing.  
No cloud hallucinations.  
No startup required.

This is **sovereign AI** â€” embedded memory, post-cloud execution, and transparent reasoning.

---

MIT License
